{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(rc={\"figure.figsize\": (20, 10)})\n",
    "plt.rc('figure', figsize=(20, 10))\n",
    "import numpy as np\n",
    "import utils;reload(utils)\n",
    "from utils import *\n",
    "from keras.layers import Merge, Permute\n",
    "import pdb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats.mstats import rankdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f1(p, r):\n",
    "    return 6 * p * r / (5 * r + p)\n",
    "def f2(p, r):\n",
    "    return 5 * p * r / (3 * p + 2 * r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = 'data_new/'\n",
    "action_cate8 = pd.read_csv(data_dir+\"train/action_type_valid.csv\", parse_dates=[\"date\"])\n",
    "users = pd.read_csv(data_dir+\"JData_User.csv\", encoding='gbk')\n",
    "\n",
    "# pos04 = pd.read_csv(\"data/train/positive04.csv\", parse_dates=[\"time_x\", \"time_y\"])\n",
    "# pos03 = pd.read_csv(\"data/train/positive03.csv\", parse_dates=[\"time_x\", \"time_y\"])\n",
    "# all_pos = pd.concat((pos03, pos04))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_pos = pd.read_csv(data_dir+\"train/positive_valid.csv\", parse_dates=[\"date_x\", \"date_y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negtive = pd.read_csv(data_dir+\"train/negative_valid.csv\", \n",
    "    parse_dates=[\"date_x\", \"date_y\"], header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_negative = pd.read_csv(data_dir+\"train/cv_negative_valid.csv\", \n",
    "    parse_dates=[\"date_x\", \"date_y\"], header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_and_neg = pd.concat((all_pos, negtive))\n",
    "# pos_and_neg = all_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "buy_train = pos_and_neg[pos_and_neg.date_x < '20160411']\n",
    "buy_cv = pos_and_neg[pos_and_neg.date_x >= '20160411']\n",
    "buy_cv = pd.concat((buy_cv, cv_negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "items = np.hstack((np.array([0]), action_cate8.sku_id.unique()))\n",
    "\n",
    "users = action_cate8.user_id.unique()\n",
    "\n",
    "userid2idx = {o:i for i,o in enumerate(users)}\n",
    "itemid2idx = {o:i for i,o in enumerate(items)}\n",
    "\n",
    "n_users = len(users)\n",
    "n_items = len(items)\n",
    "n_factors = 50\n",
    "n_users, n_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_array(data_dir+'users', users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(data_dir+'products', items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "targets = action_cate8[action_cate8.action_type_4 > 0].sku_id.unique()\n",
    "targetid2idx = {o:i for i,o in enumerate(targets)}\n",
    "n_targets = len(targets)\n",
    "n_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_days = 10\n",
    "days = 10\n",
    "top_skus = 10\n",
    "filter_day = 3\n",
    "filter_sku = 2\n",
    "days_offset = total_days-days\n",
    "\n",
    "type_select = [1,2,3,5,6,7,8,9,10,11,12]\n",
    "type_select_map = range(2, buy_cnn2d_trn.shape[-1])#[i-1+2 for i in type_select]\n",
    "type_select_count = len(type_select_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embedding_input(name, n_in, n_out, reg):\n",
    "    inp = Input(shape=(1,), dtype='int64', name=name)\n",
    "    return inp, Embedding(n_in, n_out, input_length=1, W_regularizer=l2(reg))(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_in, user_emb = embedding_input('user_in', n_users, n_factors, 1e-4)\n",
    "item_in, it = embedding_input('item_in', n_items, n_factors, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sku_acc_by_day = action_cate8.groupby(['sku_id', 'date'], as_index=False).sum()\n",
    "del sku_acc_by_day['user_id']\n",
    "del sku_acc_by_day['active_users']\n",
    "del sku_acc_by_day['user_count_for_sku']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sku_acc_by_day.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "buy_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构造训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "buy_cnn2d_trn, buy_cnn2d_trn_label = create_2dcnn_dataset(pd.merge(buy_train, \n",
    "        sku_acc_by_day, left_on = ['sku_id_y', 'date_y'], right_on=['sku_id', 'date']))\n",
    "\n",
    "buy_cnn2d_cv, buy_cnn2d_cv_label = create_2dcnn_dataset(pd.merge(buy_cv, \n",
    "        sku_acc_by_day, left_on = ['sku_id_y', 'date_y'], right_on=['sku_id', 'date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_array(data_dir+'buy_cnn2d_trn', buy_cnn2d_trn)\n",
    "save_array(data_dir+'buy_cnn2d_trn_label', buy_cnn2d_trn_label)\n",
    "save_array(data_dir+'buy_cnn2d_cv', buy_cnn2d_cv)\n",
    "save_array(data_dir+'buy_cnn2d_cv_label', buy_cnn2d_cv_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "buy_cnn2d_trn = load_array(data_dir+'buy_cnn2d_trn')\n",
    "buy_cnn2d_trn_label = load_array(data_dir+'buy_cnn2d_trn_label')\n",
    "buy_cnn2d_cv = load_array(data_dir+'buy_cnn2d_cv')\n",
    "buy_cnn2d_cv_label = load_array(data_dir+'buy_cnn2d_cv_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_feature = utils.get_basic_user_feat(data_dir)\n",
    "\n",
    "user_feature_dict = {}\n",
    "for row in user_feature.values:\n",
    "    user_feature_dict[row[0]] = row[1:]\n",
    "# user_feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "product_feat = utils.get_basic_product_feat(data_dir)\n",
    "\n",
    "product_feat_dict = {}\n",
    "for row in product_feat.values:\n",
    "    product_feat_dict[row[0]] = list(row[3:])\n",
    "\n",
    "product_feat_dict[0] = [0]*(product_feat.shape[1]-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "buy_user_feat_trn = np.array([user_feature_dict[users[int(uidx)]] for uidx in buy_cnn2d_trn[:, 0, 0, 0]])\n",
    "buy_user_feat_cv = np.array([user_feature_dict[users[int(uidx)]] for uidx in buy_cnn2d_cv[:, 0, 0, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使验证集的正负比例更接近于真实"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练时使用正负比例接近的数据集，而验证时构造正负样本更接近于真实的比例，即负样本远大于正样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "buy_cnn2d_cv[buy_cnn2d_cv_label[:, 1] == 1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_neg = buy_cnn2d_cv[buy_cnn2d_cv_label[:, 1] == 0]\n",
    "# cv_pos_unused, cv_pos = train_test_split(buy_cnn2d_cv[buy_cnn2d_cv_label[:, 1] == 1], test_size=len(cv_neg)/40)\n",
    "cv_pos = buy_cnn2d_cv[buy_cnn2d_cv_label[:, 1] == 1]\n",
    "buy_cnn2d_cv_imitate = np.vstack((cv_pos, cv_neg))\n",
    "buy_cnn2d_cv_imitate_label = np.vstack((np.ones((len(cv_pos), 2)), np.zeros((len(cv_neg), 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_pos.shape, cv_neg.shape#, cv_pos_unused.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "buy_cnn2d_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "buy_user_feat_cv_imitate = np.array([user_feature_dict[users[int(uidx)]] for uidx in buy_cnn2d_cv_imitate[:, 0, 0, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 行为热力图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "idx = 1308\n",
    "sns.heatmap(sku_cnn2d_trn[idx, :, :, 12].astype(np.float32), annot=True)\n",
    "\n",
    "buy_cnn2d_trn[idx, :, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for t in type_select_map:\n",
    "    plt.figure()\n",
    "    sns.plt.title(t-2+1)\n",
    "    sns.heatmap(np.mean(cv_pos_unused[20:21, :, :, t], 0), annot=True)\n",
    "# sku_cnn2d_trn[-1, :, :, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 购买商品集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sku_index = (buy_cnn2d_trn_label[:, 1] == 1)\n",
    "sku_cv_index = (buy_cnn2d_cv_label[:, 1] == 1)\n",
    "sku_cnn2d_trn, sku_cnn2d_trn_label = buy_cnn2d_trn[sku_index], buy_cnn2d_trn_label[sku_index]\n",
    "sku_cnn2d_cv, sku_cnn2d_cv_label = buy_cnn2d_cv[sku_cv_index], buy_cnn2d_cv_label[sku_cv_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sku_user_feat_trn = np.array([user_feature_dict[users[int(uidx)]] for uidx in sku_cnn2d_trn[:, 0, 0, 0]])\n",
    "sku_user_feat_cv = np.array([user_feature_dict[users[int(uidx)]] for uidx in sku_cnn2d_cv[:, 0, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sku_user_feat_trn.shape, sku_user_feat_cv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全连接神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trn, trn_label = create_dataset(train)\n",
    "val, val_label = create_dataset(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_type():\n",
    "    inp = Input(shape=(1,12), dtype='float32')\n",
    "    return inp, Dense(24)(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type_in, tp = create_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bias(inp, n_in):\n",
    "    x = Embedding(n_in, 1, input_length=1)(inp)\n",
    "    return Flatten()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ub = create_bias(user_in, n_users)\n",
    "uit = create_bias(item_in, n_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练购买商品模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# , cnn_trn_label[:, 1]\n",
    "# , cnn_cv_label[:, 1]\n",
    "# , sku_cnn2d_trn[:, 0, 0, 0]\n",
    "# , sku_cnn2d_cv[:, 0, 0, 0]\n",
    "epoch=4\n",
    "conv2d_sku.compile(Adam(lr=0.01), loss=['sparse_categorical_crossentropy'],\n",
    "           metrics=['accuracy'])\n",
    "conv2d_sku.fit([sku_cnn2d_trn[:, days_offset:, :top_skus, 1:2], sku_cnn2d_trn[:, days_offset:, :top_skus, type_select_map], sku_user_feat_trn], [sku_cnn2d_trn_label[:, 0]], \n",
    "            batch_size=64, nb_epoch=epoch,\n",
    "            validation_data=([sku_cnn2d_cv[:, days_offset:, :top_skus, 1:2], sku_cnn2d_cv[:, days_offset:, :top_skus, type_select_map], sku_user_feat_cv],\n",
    "                             [sku_cnn2d_cv_label[:, 0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练是否购买模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# , buy_cnn2d_trn[:, 0, 0, 0]\n",
    "# , buy_cnn2d_cv[:, 0, 0, 0]\n",
    "# buy_cnn2d_trn[:, (total_days-days):, :top_skus, 1:2], \n",
    "# buy_cnn2d_cv[:, (total_days-days):, :top_skus, 1:2], \n",
    "epoch = 2\n",
    "conv2d_buy.compile(Adam(lr=0.01), loss=['sparse_categorical_crossentropy', 'binary_crossentropy'],\n",
    "           metrics=['accuracy'], loss_weights=[0.01, 1.])\n",
    "conv2d_buy.fit([buy_cnn2d_trn[:, days_offset:, :top_skus, 1:2], buy_cnn2d_trn[:, days_offset:, :top_skus, type_select_map].astype(np.float32), buy_user_feat_trn],\n",
    "               [buy_cnn2d_trn_label[:, 0], buy_cnn2d_trn_label[:, 1]], \n",
    "            batch_size=512, nb_epoch=epoch,\n",
    "            validation_data=([buy_cnn2d_cv_imitate[:, days_offset:, :top_skus, 1:2], buy_cnn2d_cv_imitate[:, days_offset:, :top_skus, type_select_map].astype(np.float32), buy_user_feat_cv_imitate],\n",
    "                             [buy_cnn2d_cv_imitate_label[:, 0], buy_cnn2d_cv_imitate_label[:, 1]]), class_weight={0: 2., 1: 1.})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证集上计算Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_pred = conv2d_buy.predict([buy_cnn2d_cv_imitate[:, days_offset:, :top_skus, 1:2], buy_cnn2d_cv_imitate[:, days_offset:, :top_skus, type_select_map], buy_user_feat_cv_imitate])\n",
    "cv_pred_df = pd.DataFrame(cv_pred[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# buy_cnn2d_cv[:, :, :, 1:2], \n",
    "\n",
    "# cv_pred_df = pd.DataFrame(np.argmax(cv_pred, 1))\n",
    "\n",
    "cv_pred_df['user_id'] = [users[int(i)] for i in list(buy_cnn2d_cv_imitate[:, 0, 0, 0])]\n",
    "\n",
    "cv_pred_df['buy'] = cv_pred_df.apply(lambda x: 1 if x[0] > 0.60 else 0, 1)\n",
    "\n",
    "cv_pred_df['true'] = buy_cnn2d_cv_imitate_label[:, 1]\n",
    "# cv_pred_df['true'] = [targets[i] for i in cnn_cv_label[:, 0]]\n",
    "\n",
    "p = len(cv_pred_df[(cv_pred_df.buy == 1) & (cv_pred_df.true == 1)]) / float(len(cv_pred_df[cv_pred_df.buy==1]))\n",
    "r = len(cv_pred_df[(cv_pred_df.buy == 1) & (cv_pred_df.true == 1)]) / float(len(cv_pred_df[cv_pred_df.true==1]))\n",
    "# p = len(cv_pred_df[cv_pred_df.buy == cv_pred_df.true]) / float(len(cv_pred_df))\n",
    "# r = len(cv_pred_df[cv_pred_df.buy == cv_pred_df.true]) / float(len(cv_pred_df))\n",
    "p, r, f1(p, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构造测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = load_array(data_dir+'test_cnn2d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(data_dir+\"train/test.csv\", parse_dates=[\"date_x\", \"date_y\"])\n",
    "\n",
    "test, _ = create_2dcnn_dataset(pd.merge(test_df, \n",
    "        sku_acc_by_day, left_on = ['sku_id_y', 'date_y'], right_on=['sku_id', 'date']))\n",
    "\n",
    "save_array(data_dir+'test_cnn2d', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_user_feat = np.array([user_feature_dict[users[int(uidx)]] for uidx in test[:, 0, 0, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对测试集做预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# , sku_cnn2d_trn[:, 0, 0, 0]\n",
    "epoch = 4\n",
    "sku_cnn2d_all = np.vstack((sku_cnn2d_trn, sku_cnn2d_cv))\n",
    "sku_cnn2d_all_label = np.vstack((sku_cnn2d_trn_label, sku_cnn2d_cv_label))\n",
    "sku_user_feat_all = np.vstack((sku_user_feat_trn, sku_user_feat_cv))\n",
    "conv2d_sku.compile(Adam(lr=0.01), loss=['sparse_categorical_crossentropy'],\n",
    "           metrics=['accuracy'])\n",
    "\n",
    "conv2d_sku.fit([sku_cnn2d_all[:, days_offset:, :top_skus, 1:2], sku_cnn2d_all[:, days_offset:, :top_skus, type_select_map], sku_user_feat_all], [sku_cnn2d_all_label[:, 0]], \n",
    "            batch_size=64, nb_epoch=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# , buy_cnn2d_all[:, 0, 0, 0]\n",
    "# buy_cnn2d_all[:, days_offset:, :top_skus, 1:2], \n",
    "epoch = 4\n",
    "buy_cnn2d_all = np.vstack((buy_cnn2d_trn, buy_cnn2d_cv))\n",
    "buy_cnn2d_all_label = np.vstack((buy_cnn2d_trn_label, buy_cnn2d_cv_label))\n",
    "buy_user_feat_all = np.vstack((buy_user_feat_trn, buy_user_feat_cv))\n",
    "conv2d_buy.compile(Adam(lr=0.01), loss=['sparse_categorical_crossentropy', 'binary_crossentropy'],\n",
    "           metrics=['accuracy'], loss_weights=[0.1, 1.])\n",
    "conv2d_buy.fit([buy_cnn2d_all[:, days_offset:, :top_skus, 1:2], buy_cnn2d_all[:, days_offset:, :top_skus, type_select_map], buy_user_feat_all], \n",
    "               [buy_cnn2d_all_label[:, 0], buy_cnn2d_all_label[:, 1]], \n",
    "            batch_size=128, nb_epoch=epoch, class_weight={0: 2., 1: 1.})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test[:, 0, 0, 0]\n",
    "# test[:, days_offset:, :top_skus, 1:2], \n",
    "buy_predicted = conv2d_buy.predict([test[:, days_offset:, :top_skus, 1:2], test[:, days_offset:, :top_skus, type_select_map], test_user_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# , test[:, 0, 0, 0]\n",
    "sku_predicted = conv2d_sku.predict([test[:, days_offset:, :top_skus, 1:2], test[:, days_offset:, :top_skus, type_select_map], test_user_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_pred_df = pd.DataFrame(buy_predicted[1])\n",
    "test_pred_df['user_id'] = [int(users[int(i)]) for i in list(test[:, 0, 0, 0])]\n",
    "test_pred_df['is_buy'] = test_pred_df.apply(lambda x: 1 if x[0] > 0.66 else 0, 1)\n",
    "test_pred_df['sku_id'] = items[np.argmax(sku_predicted, 1)]\n",
    "test_pred_df.is_buy.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_df = test_pred_df[(test_pred_df.is_buy==1)&(test_pred_df.sku_id!=0)][['user_id', 'sku_id']]\n",
    "print len(result_df)\n",
    "result_df.to_csv(\"cnn2d.csv\", index=False)\n",
    "from IPython.display import FileLink\n",
    "FileLink('cnn2d.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_2dcnn_dataset(pd.merge(buy_cv.tail(100), sku_acc_by_day, left_on = ['sku_id_y', 'date_y'], right_on=['sku_id', 'date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_2dcnn_dataset(df):\n",
    "    dual_group = df.groupby(['user_id', 'sku_id_x', 'window_start'])\n",
    "    feature = []\n",
    "    label = []\n",
    "    for (uid, sid_buy, window_start), g in dual_group:\n",
    "#         print time_buy\n",
    "        sum_of_recent = g.sum()[['action_type_1_x',\n",
    "                 'action_type_2_x',\n",
    "                 'action_type_3_x',\n",
    "                 'action_type_4_x',\n",
    "                 'action_type_5_x',\n",
    "                 'action_type_6_x']].values + 1 #近10天该用户对所有商品的交互总和\n",
    "    \n",
    "        window = []\n",
    "        date_range = pd.date_range(start=str(window_start), periods=10, freq='D',closed='left')\n",
    "#         print date_range\n",
    "#         pdb.set_trace()\n",
    "        for d in date_range:\n",
    "            day_top10 = []\n",
    "            window_day = d.strftime(\"%Y%m%d\")\n",
    "#             pdb.set_trace()\n",
    "            # 取一天之中action最多的商品\n",
    "            sku_of_day = g[g.date == window_day] #该用户在一天中所有商品交互记录\n",
    "            \n",
    "            if len(sku_of_day) == 0:\n",
    "                top_skus = []\n",
    "                sum_of_day = np.ones(6)\n",
    "            else:\n",
    "                sum_of_day = sku_of_day.sum()[['action_type_1_x',\n",
    "                 'action_type_2_x',\n",
    "                 'action_type_3_x',\n",
    "                 'action_type_4_x',\n",
    "                 'action_type_5_x',\n",
    "                 'action_type_6_x']].values + 1 # 一天中该用户对所有商品的交互总和\n",
    "                \n",
    "                top_skus = sku_of_day.sort_values('action_type_1_x', ascending=False)\\\n",
    "                [['sku_id_y', \n",
    "                 'action_type_1_x',\n",
    "                 'action_type_2_x',\n",
    "                 'action_type_3_x',\n",
    "                 'action_type_4_x',\n",
    "                 'action_type_5_x',\n",
    "                 'action_type_6_x',\n",
    "                 'action_type_1_y',\n",
    "                 'action_type_2_y',\n",
    "                 'action_type_3_y',\n",
    "                 'action_type_4_y',\n",
    "                 'action_type_5_y',\n",
    "                 'action_type_6_y',\n",
    "                 'user_count_for_sku',\n",
    "                 'active_users'\n",
    "                 ]].values[:10] #一天中该用户排名前十的交互商品的交互量\n",
    "                \n",
    "            for i in range(10):\n",
    "                try:\n",
    "                    sku = top_skus[i]\n",
    "                except:\n",
    "                    sku = np.array([0] * 15)\n",
    "                \n",
    "                sku_id = int(itemid2idx[sku[0]])\n",
    "                sku_action_count = sku[1:7] #该用户当天对该种商品的各类交互量\n",
    "                sku_action_all = sku[7:13] #一天当中所有用户对某种商品的交互总和\n",
    "#                 pdb.set_trace()\n",
    "                idf = 1.0 / ((sku[13] + 1) / (sku[14] + 1))\n",
    "                \n",
    "                sku_feat_day = np.hstack((np.array([int(userid2idx[uid])]),\n",
    "                           np.array([sku_id]),\n",
    "                           np.array(product_feat_dict[sku[0]]),\n",
    "                           sku_action_count,\n",
    "                           np.array([sku_action_all[3] / (sku_action_all[i] + 1) for i in [0,1,2,4,5]]),\n",
    "                                          #该种商品的购买数和各类交互量之比\n",
    "                           sku_action_count / (sku_action_all + 1),\n",
    "                           sku_action_count / (np.sum(sku_action_count) + 1.0), #对一种商品各种交互的占比\n",
    "                           sku_action_count / sum_of_recent,\n",
    "                           sku_action_count / sum_of_day * idf\n",
    "                          ))\n",
    "                    \n",
    "                day_top10.append(sku_feat_day)\n",
    "                \n",
    "            window.append(day_top10)\n",
    "        feature.append(window)\n",
    "        label.append([itemid2idx[sid_buy], 0 if sid_buy == 0 else 1])\n",
    "    return np.array(feature), np.array(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "conv_item_model = Sequential([\n",
    "    Embedding(n_items, 32, input_length=10, W_regularizer=l2(1e-4)),\n",
    "    Dropout(0.25),\n",
    "    BatchNormalization(),\n",
    "    Convolution1D(32, 3, border_mode='same', activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    MaxPooling1D(),\n",
    "    Convolution1D(32, 3, border_mode='same', activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    MaxPooling1D(),\n",
    "    Flatten(),\n",
    "    Dense(100)\n",
    "])\n",
    "conv_item_model.summary()\n",
    "conv_type_model = Sequential([\n",
    "#     Dense(32, input_shape=(10, 12)),\n",
    "#     Dropout(0.25),\n",
    "    Convolution1D(32, 3, border_mode='same', activation='relu', input_length=10, input_shape=(10,6)),\n",
    "    Dropout(0.2),\n",
    "    MaxPooling1D(),\n",
    "    Convolution1D(32, 3, border_mode='same', activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    MaxPooling1D(),\n",
    "    Flatten(),\n",
    "    Dense(100)\n",
    "])\n",
    "\n",
    "conv_item_in = Input(shape=(10,), dtype='int64', name='conv_item_in')\n",
    "conv_item = conv_item_model(conv_item_in)\n",
    "# conv_item_emb = Embedding(n_items, 32, input_length=10, W_regularizer=l2(1e-4))(conv_item_in)\n",
    "\n",
    "conv_type_in = Input(shape=(10,6), dtype='float32', name='conv_type_in')\n",
    "conv_type = BatchNormalization()(conv_type_in)\n",
    "conv_type = conv_type_model(conv_type_in)\n",
    "# conv_type = Dense(32, input_shape=(10,12))(conv_type_in)\n",
    "\n",
    "# out = Merge(mode='concat', concat_axis=2)([conv_item_emb, conv_type])\n",
    "# graph = Model([conv_item_in, conv_type_in], out)\n",
    "\n",
    "# conv1 = Sequential([\n",
    "#     graph,\n",
    "#     Convolution1D(20, 3, border_mode='same', activation='relu'),\n",
    "#     Dropout(0.2),\n",
    "#     MaxPooling1D(),\n",
    "#     Flatten(),\n",
    "#     Dense(100, activation='relu'),\n",
    "#     Dropout(0.7)\n",
    "# ])\n",
    "user_in, user_emb = embedding_input('user_in', n_users, n_factors, 1e-4)\n",
    "user_emb = Flatten()(user_emb)\n",
    "\n",
    "conv1 = merge([conv_item, conv_type, user_emb], mode='concat')\n",
    "\n",
    "buy_x = Dense(1, activation='sigmoid', name=\"buy\")(conv1)\n",
    "sku_x = Dense(n_targets, activation='softmax', name=\"sku\")(conv1)\n",
    "\n",
    "# conv_all = Model([conv_item_in, conv_type_in, user_in], [sku_x, buy_x])\n",
    "conv_all = Model([conv_item_in, conv_type_in, user_in], sku_x)\n",
    "# conv_all.compile(Adam(lr=0.001), loss=['sparse_categorical_crossentropy', 'binary_crossentropy'],\n",
    "#            metrics=['accuracy'], loss_weights=[0.01, 1.])\n",
    "conv_all.compile(Adam(lr=0.001), loss=['sparse_categorical_crossentropy'],\n",
    "           metrics=['accuracy'])\n",
    "\n",
    "from keras.utils.visualize_util import plot\n",
    "plot(conv_all, to_file='conv_all.png', show_shapes=True)\n",
    "from IPython.display import Image\n",
    "Image(filename='conv_all.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D CNN For 购买商品预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# n_items = 4028\n",
    "conv2d_item_model = Sequential([\n",
    "    Embedding(n_items, 10, input_length=days*top_skus, W_regularizer=l2(1e-4)),\n",
    "    Reshape((days,top_skus,4)),\n",
    "    Dropout(0.25),\n",
    "    BatchNormalization(),\n",
    "    Permute((3, 1, 2)),\n",
    "    Convolution2D(4, filter_day, filter_sku, dim_ordering='th', border_mode='same', activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    MaxPooling2D(),\n",
    "    Convolution2D(4, filter_day, filter_sku, dim_ordering='th', border_mode='same', activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    MaxPooling2D(),\n",
    "    Flatten(),\n",
    "    Dense(64)\n",
    "])\n",
    "conv2d_item_in = Input(shape=(days,top_skus,1), dtype='int64', name='conv2d_item_in')\n",
    "conv2d_item = Flatten()(conv2d_item_in)\n",
    "conv2d_item = conv2d_item_model(conv2d_item)\n",
    "\n",
    "conv2d_type_model = Sequential([\n",
    "#     Dense(32, input_shape=(type_select_count,days,top_skus), activation='relu'),\n",
    "#     Dropout(0.25),\n",
    "    Convolution2D(32, filter_day, filter_sku, dim_ordering='th', border_mode='valid', activation='relu', \n",
    "                  input_shape=(type_select_count,days,top_skus)),\n",
    "    Dropout(0.7),\n",
    "#     MaxPooling2D(),\n",
    "    Convolution2D(32, filter_day, filter_sku, dim_ordering='th', border_mode='valid', activation='relu'),\n",
    "    Dropout(0.2),\n",
    "#     MaxPooling2D(),\n",
    "#     Flatten(),\n",
    "#     Dense(100)\n",
    "    Flatten(),\n",
    "#     Dense(128, activation='relu'),\n",
    "#     Dropout(0.7),\n",
    "#     BatchNormalization(),\n",
    "#     Dense(128, activation='relu'),\n",
    "#     Dropout(0.7),\n",
    "#     BatchNormalization(),\n",
    "#     Dense(64, activation='relu')\n",
    "])\n",
    "\n",
    "# conv_item_emb = Embedding(n_items, 32, input_length=10, W_regularizer=l2(1e-4))(conv_item_in)\n",
    "\n",
    "conv2d_type_in = Input(shape=(days,top_skus,type_select_count), dtype='float32', name='conv2d_type_in')\n",
    "conv2d_type = BatchNormalization()(conv2d_type_in)\n",
    "conv2d_type = Permute((3, 1, 2))(conv2d_type)\n",
    "conv2d_type = conv2d_type_model(conv2d_type)\n",
    "\n",
    "# conv_type = Dense(32, input_shape=(10,12))(conv_type_in)\n",
    "\n",
    "# out = Merge(mode='concat', concat_axis=2)([conv_item_emb, conv_type])\n",
    "# graph = Model([conv_item_in, conv_type_in], out)\n",
    "\n",
    "# conv1 = Sequential([\n",
    "#     graph,\n",
    "#     Convolution1D(20, 3, border_mode='same', activation='relu'),\n",
    "#     Dropout(0.2),\n",
    "#     MaxPooling1D(),\n",
    "#     Flatten(),\n",
    "#     Dense(100, activation='relu'),\n",
    "#     Dropout(0.7)\n",
    "# ])\n",
    "# user_in, user_emb = embedding_input('user_in', n_users, n_factors, 1e-4)\n",
    "# user_emb = Flatten()(user_emb)\n",
    "user_feat_in = Input(dtype='float32', shape=(sku_user_feat_trn.shape[1],), name='user_feat_in')\n",
    "user_feat_dense = Dense(64, activation='relu')(user_feat_in)\n",
    "\n",
    "# conv2d = merge([conv2d_item, conv2d_type, user_emb], mode='concat')\n",
    "conv2d = merge([conv2d_item, conv2d_type, user_feat_dense], mode='concat')\n",
    "# conv2d = Dense(32, activation='relu')(conv2d)\n",
    "# conv2d = Dropout(0.7)(conv2d)\n",
    "\n",
    "buy_x = Dense(1, activation='sigmoid', name=\"buy\")(conv2d)\n",
    "sku_x = Dense(n_items, activation='softmax', name=\"sku\")(conv2d)\n",
    "\n",
    "# conv_all = Model([conv_item_in, conv_type_in, user_in], [sku_x, buy_x])\n",
    "# conv2d_all = Model([conv2d_item_in, conv2d_type_in, user_in], sku_x)\n",
    "conv2d_sku = Model([conv2d_item_in, conv2d_type_in, user_feat_in], sku_x)\n",
    "# conv_all.compile(Adam(lr=0.001), loss=['sparse_categorical_crossentropy', 'binary_crossentropy'],\n",
    "#            metrics=['accuracy'], loss_weights=[0.01, 1.])\n",
    "conv2d_sku.compile(Adam(lr=0.001), loss=['sparse_categorical_crossentropy'],\n",
    "           metrics=['accuracy'])\n",
    "\n",
    "from keras.utils.visualize_util import plot\n",
    "plot(conv2d_sku, to_file='conv2d_sku.png', show_shapes=True)\n",
    "from IPython.display import Image\n",
    "Image(filename='conv2d_sku.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 是否购买"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv2d_item_model = Sequential([\n",
    "    Embedding(n_items, 10, input_length=days*top_skus, W_regularizer=l2(1e-4)),\n",
    "    Reshape((days,top_skus,10)),\n",
    "    Dropout(0.25),\n",
    "    BatchNormalization(),\n",
    "    Permute((3, 1, 2)),\n",
    "    Convolution2D(5, filter_day, filter_sku, dim_ordering='th', border_mode='same', activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    MaxPooling2D(),\n",
    "    Convolution2D(5, filter_day, filter_sku, dim_ordering='th', border_mode='same', activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    MaxPooling2D(),\n",
    "    Flatten(),\n",
    "    Dense(64)\n",
    "])\n",
    "conv2d_item_in = Input(shape=(days,top_skus,1), dtype='int64', name='conv2d_item_in')\n",
    "conv2d_item = Flatten()(conv2d_item_in)\n",
    "conv2d_item = conv2d_item_model(conv2d_item)\n",
    "\n",
    "conv2d_type_model = Sequential([\n",
    "#     Dense(32, input_shape=(type_select_count,days,top_skus), activation='relu'),\n",
    "#     Dropout(0.25),\n",
    "    Convolution2D(5, filter_day, filter_sku, dim_ordering='th', border_mode='valid', activation='relu', \n",
    "                  input_shape=(type_select_count,days,top_skus)),\n",
    "    Dropout(0.7),\n",
    "#     MaxPooling2D(),\n",
    "    Convolution2D(5, filter_day, filter_sku, dim_ordering='th', border_mode='valid', activation='relu'),\n",
    "    Dropout(0.2),\n",
    "#     MaxPooling2D(),\n",
    "#     Flatten(),\n",
    "#     Dense(100)\n",
    "    Flatten(),\n",
    "#     Dense(128, activation='relu'),\n",
    "#     Dropout(0.7),\n",
    "#     BatchNormalization(),\n",
    "#     Dense(128, activation='relu'),\n",
    "#     Dropout(0.7),\n",
    "#     BatchNormalization(),\n",
    "#     Dense(64, activation='relu')\n",
    "])\n",
    "\n",
    "# conv_item_emb = Embedding(n_items, 32, input_length=10, W_regularizer=l2(1e-4))(conv_item_in)\n",
    "\n",
    "conv2d_type_in = Input(shape=(days,top_skus,type_select_count), dtype='float32', name='conv2d_type_in')\n",
    "conv2d_type = BatchNormalization()(conv2d_type_in)\n",
    "conv2d_type = Permute((3, 1, 2))(conv2d_type)\n",
    "conv2d_type = conv2d_type_model(conv2d_type)\n",
    "\n",
    "# conv_item_emb = Embedding(n_items, 32, input_length=10, W_regularizer=l2(1e-4))(conv_item_in)\n",
    "\n",
    "conv2d_type_in = Input(shape=(days,top_skus,type_select_count), dtype='float32', name='conv2d_type_in')\n",
    "conv2d_type = BatchNormalization()(conv2d_type_in)\n",
    "conv2d_type = Permute((3, 1, 2))(conv2d_type)\n",
    "conv2d_type = conv2d_type_model(conv2d_type)\n",
    "\n",
    "# conv_type = Dense(32, input_shape=(10,12))(conv_type_in)\n",
    "\n",
    "# out = Merge(mode='concat', concat_axis=2)([conv_item_emb, conv_type])\n",
    "# graph = Model([conv_item_in, conv_type_in], out)\n",
    "\n",
    "# conv1 = Sequential([\n",
    "#     graph,\n",
    "#     Convolution1D(20, 3, border_mode='same', activation='relu'),\n",
    "#     Dropout(0.2),\n",
    "#     MaxPooling1D(),\n",
    "#     Flatten(),\n",
    "#     Dense(100, activation='relu'),\n",
    "#     Dropout(0.7)\n",
    "# ])\n",
    "# user_in, user_emb = embedding_input('user_in', n_users, n_factors, 1e-4)\n",
    "# user_emb = Flatten()(user_emb)\n",
    "user_feat_in = Input(dtype='float32', shape=(user_feature.shape[1] - 1,), name='user_feat_in')\n",
    "user_feat_dense = Dense(64, activation='relu')(user_feat_in)\n",
    "\n",
    "# conv2d = merge([conv2d_item, conv2d_type, user_emb], mode='concat')\n",
    "conv2d = merge([conv2d_item, conv2d_type, user_feat_dense], mode='concat')\n",
    "# conv2d = Dense(32, activation='relu')(conv2d)\n",
    "# conv2d = Dropout(0.7)(conv2d)\n",
    "\n",
    "buy_x = Dense(1, activation='sigmoid', name=\"buy\")(conv2d)\n",
    "sku_x = Dense(n_items, activation='softmax', name=\"sku\")(conv2d)\n",
    "# conv_all = Model([conv_item_in, conv_type_in, user_in], [sku_x, buy_x])\n",
    "# conv2d_buy = Model([conv2d_item_in, conv2d_type_in, user_in], buy_x)\n",
    "conv2d_buy = Model([conv2d_item_in, conv2d_type_in, user_feat_in], [sku_x, buy_x])\n",
    "conv2d_buy.compile(Adam(lr=0.001), loss=['sparse_categorical_crossentropy', 'binary_crossentropy'],\n",
    "           metrics=['accuracy'], loss_weights=[0.1, 1.])\n",
    "# conv2d_buy.compile(Adam(lr=0.001), loss=['binary_crossentropy'],\n",
    "#            metrics=['accuracy'])\n",
    "\n",
    "from keras.utils.visualize_util import plot\n",
    "plot(conv2d_buy, to_file='conv2d_buy.png', show_shapes=True)\n",
    "from IPython.display import Image\n",
    "Image(filename='conv2d_buy.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
