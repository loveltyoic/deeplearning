{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_train = pd.read_csv(\"dataSets/training/trajectories(table 5)_training.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "routes = [u'A-2', u'A-3', u'B-1', u'B-3', u'C-1', u'C-3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def map_time_window(r, field):\n",
    "    stime = datetime.strptime(r[field], \"%Y-%m-%d %H:%M:%S\")\n",
    "    time_window_start = datetime(stime.year, stime.month, stime.day, stime.hour, stime.minute / 20 * 20)\n",
    "    return time_window_start\n",
    "    \n",
    "time_train['window'] = time_train.apply(lambda r: map_time_window(r, 'starting_time'), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def route(r):\n",
    "    return r['intersection_id'] + '-' + str(r['tollgate_id'])\n",
    "time_train['route'] = time_train.apply(lambda r: route(r), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_train.index = time_train['window']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history_mean = time_train.groupby([lambda x: (x.isoweekday(), x.hour, x.minute), 'route'])['travel_time'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hour_minute_mean = time_train.groupby([lambda x: (x.hour, x.minute), 'route'])['travel_time'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "travel_time_feature = time_train.groupby([lambda x: x, 'route'])['travel_time'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaler.fit(time_train.travel_time.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# scaler.transform(time_train.travel_time.values.reshape(-1, 1)).reshape(-1)\n",
    "# time_train.travel_time.values\n",
    "# time_train.groupby([lambda x: x, 'route'])['travel_time'].mean().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Travel Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_file_path = 'test_20min_avg_travel_time.csv'\n",
    "fw = open(out_file_path, 'w')\n",
    "fw.writelines(','.join(['intersection_id', 'tollgate_id', 'time_window', 'avg_travel_time']) + '\\n')\n",
    "for route in routes:\n",
    "    for day in range(18, 25):\n",
    "        for hour in [8, 17]:\n",
    "            time_window_start = datetime(2016, 10, day, hour, 0)\n",
    "            time_window_end = time_window_start + timedelta(hours=2)\n",
    "            for t in pd.date_range(time_window_start, end=time_window_end, freq='20min', closed='left'):\n",
    "\n",
    "                avg_tt = history_mean[(t.isoweekday(), t.hour, t.minute), route]\n",
    "                out_line = ','.join([route.split('-')[0], route.split('-')[1],\n",
    "                                 '\"[' + str(t) + ',' + str(t + timedelta(minutes=20)) + ')\"',\n",
    "                                 str(avg_tt)]) + '\\n'\n",
    "                fw.writelines(out_line)\n",
    "fw.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink(out_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "volume_train = pd.read_csv(\"dataSets/training/volume(table 6)_training.csv\", header=0)\n",
    "volume_train['window'] = volume_train.apply(lambda r: map_time_window(r, 'time'), 1)\n",
    "volume_train.index = volume_train['window']\n",
    "volume_history_grouped = volume_train.groupby([lambda x: (x.isoweekday(), x.hour, x.minute), 'tollgate_id', 'direction'])\n",
    "volume_history_mean = volume_history_grouped['window'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "volume_test_file_path = \"test_20min_avg_volume.csv\"\n",
    "fw = open(volume_test_file_path, 'w')\n",
    "fw.writelines(','.join(['tollgate_id', 'time_window', 'direction', 'volume']) + '\\n')\n",
    "\n",
    "for day in range(18, 25):\n",
    "    for hour in [8, 15]:\n",
    "        time_window_start = datetime(2016, 10, day, hour, 0)\n",
    "        time_window_end = time_window_start + timedelta(hours=2)\n",
    "        for t in pd.date_range(time_window_start, end=time_window_end, freq='20min', closed='left'):\n",
    "            for tollgate_id in [1, 2, 3]:\n",
    "                for direction in [0, 1]:\n",
    "                    try:\n",
    "                        volume = volume_history_mean[(t.isoweekday(), t.hour, t.minute), tollgate_id, direction]\n",
    "                        out_line = ','.join([str(tollgate_id), \n",
    "                                         '\"[' + str(t) + ',' + str(t+timedelta(minutes=20)) + ')\"',\n",
    "                                         str(direction),\n",
    "                                         str(volume)\n",
    "                                       ]) + '\\n'\n",
    "                        fw.writelines(out_line)\n",
    "                    except KeyError:\n",
    "                        pass                         \n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink(volume_test_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation - Travel Time by History Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用历史平均作为待预测的通行时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set = time_train[:'201610110000']\n",
    "validation_set = time_train['201610110000':]\n",
    "\n",
    "validation_true = pd.DataFrame(validation_set.groupby([lambda x: x, 'route'])['travel_time'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set_mean = train_set.groupby([lambda x: (x.isoweekday(), x.hour, x.minute), 'route'])['travel_time'].mean()\n",
    "validation_day = range(11, 18)\n",
    "validation_hour = [8, 17]\n",
    "validation_travel_time = []\n",
    "validation_start_window = []\n",
    "for day in validation_day:\n",
    "    for hour in validation_hour:\n",
    "        time_window_start = datetime(2016, 10, day, hour, 0)\n",
    "        time_window_end = time_window_start + timedelta(hours=2)\n",
    "        for t in pd.date_range(time_window_start, end=time_window_end, freq='20min', closed='left'):\n",
    "            for route in [u'A-2', u'A-3', u'B-1', u'B-3', u'C-1', u'C-3']:\n",
    "                avg_tt = train_set_mean[(t.isoweekday(), t.hour, t.minute), route]\n",
    "                validation_travel_time.append([avg_tt, route])\n",
    "                validation_start_window.append(t)\n",
    "\n",
    "validation_predict = pd.DataFrame(validation_travel_time, index=validation_start_window, columns=['travel_time', 'route'])\n",
    "validation_predict = pd.DataFrame(validation_predict.groupby([lambda x: x, 'route'])['travel_time'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cross_validation = pd.merge(validation_predict, validation_true, left_index=True, right_index=True, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mape(ypredict, ytrue):\n",
    "    return np.mean(np.abs(ypredict - ytrue) / ytrue)\n",
    "\n",
    "print 'MAPE of History Mean: %f' % (mape(cross_validation.values[:, 0], cross_validation.values[:, 1]))\n",
    "cross_validation.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc('figure', figsize=(20,10))\n",
    "sample_route = 'C-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# routes =  [u'A-2', u'A-3', u'B-1', u'B-3', u'C-1', u'C-3']\n",
    "def create_dataset(dataframe, route, travel_time_set, look_back=1, predict_forward=1):\n",
    "    dataset_time_index = []\n",
    "    dataset_raw = []\n",
    "    for t in pd.date_range(dataframe.index[0], dataframe.index[-1], freq='20min', closed='left'):\n",
    "        if t.hour > 5:\n",
    "            try:\n",
    "                history_feature = history_mean[(t.isoweekday(), t.hour, t.minute), route]\n",
    "            except KeyError:\n",
    "                history_feature = hour_minute_mean[(t.hour, t.minute), route]\n",
    "                 #         day_feature = [1 if i == t.isoweekday() else 0 for i in range(1,8)] + [t.hour, t.minute]\n",
    "#             day_feature = [t.isoweekday(), t.hour, t.minute, 1 if t.isoweekday() in [6,7] else 0]\n",
    "        #         day_feature = [t.isoweekday()]\n",
    "            day_feature = [t.hour]\n",
    "        #         feature = [speed, weather[t.strftime('%Y-%m-%d')]] + day_feature\n",
    "            try:\n",
    "                travel_time_stat = [0.0 if math.isnan(f) else f \\\n",
    "                                    for f in travel_time_set[t, route][['mean', 'count', 'std', '50%']].values]\n",
    "                if travel_time_stat[0] > 2 * history_feature:\n",
    "                    feature = [history_feature] + day_feature\n",
    "                else:\n",
    "                    feature = [travel_time_stat[0]] + day_feature\n",
    "            except:\n",
    "                if len(dataset_raw) < 1:\n",
    "                    feature = [history_mean[(t.isoweekday(), t.hour, t.minute), route]] + day_feature\n",
    "                else:\n",
    "                    feature = dataset_raw[-1]\n",
    "\n",
    "            dataset_raw.append(feature)\n",
    "            dataset_time_index.append(t)\n",
    "\n",
    "    dataset = np.array(dataset_raw)\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    dataset[:, 0] = scaler.fit_transform(dataset[:, 0].reshape(-1, 1)).reshape(-1)\n",
    "#     dataset[:, 0] = scaler.transform(dataset[:, 0].reshape(-1, 1)).reshape(-1)\n",
    "    dataX, dataY, timeIndexY = [], [], []\n",
    "    for i in range(len(dataset) - look_back - predict_forward + 1):\n",
    "\n",
    "        a = dataset[i:(i+look_back), :]\n",
    "\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[(i+look_back):(i+look_back+predict_forward), 0])\n",
    "        timeIndexY.append(dataset_time_index[(i+look_back):(i+look_back+predict_forward)])\n",
    "    return (np.array(dataX), np.array(dataY), scaler, timeIndexY, \n",
    "            pd.DataFrame(scaler.inverse_transform(dataset[:, 0]), index=dataset_time_index, columns=[\"speed\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train and CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_test_split_date = '20161011'\n",
    "train, cv = time_train[:train_test_split_date], time_train[train_test_split_date:]\n",
    "print(len(train), len(cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "look_back = 6\n",
    "trainX, trainY, trainScaler, trainYTimeIndex, trainDF = create_dataset(train, sample_route, travel_time_feature, look_back)\n",
    "# testX, testY, testScaler = create_dataset(test, look_back)['A-2']\n",
    "\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], trainX.shape[2]))\n",
    "# testX = np.reshape(testX, (testX.shape[0], testX.shape[1], testX.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plt.plot(trainScaler.inverse_transform(trainY))\n",
    "trainYDataFrame = pd.DataFrame(trainScaler.inverse_transform(trainY), index=np.array(trainYTimeIndex).reshape(-1))\n",
    "trainYDataFrame.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model of LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.regularizers import l2, activity_l2\n",
    "hidden_neurons = 4\n",
    "loss_function = 'mse'\n",
    "batch_size = 100\n",
    "dropout = 0.02\n",
    "inner_hidden_neurons = 8\n",
    "dropout_inner = 0.02\n",
    "out_neurons = 1\n",
    "model = Sequential()\n",
    "# model.add(LSTM(4, batch_input_shape=(batch_size, look_back, trainX.shape[2]), activation='tanh', dropout_U=0.05, stateful=True, return_sequences=True))\n",
    "# model.add(LSTM(8, batch_input_shape=(batch_size, look_back, trainX.shape[2]), activation='tanh', dropout_U=0.05, stateful=True))\n",
    "# model.add(Dense(1))\n",
    "# for i in range(100):\n",
    "#     model.fit(trainX, trainY, nb_epoch=1, batch_size=batch_size, verbose=2, shuffle=False)\n",
    "#     model.reset_states()\n",
    "in_neurons = trainX.shape[2]\n",
    "gpu_cpu = 'cpu'\n",
    "model.add(LSTM(output_dim=hidden_neurons, input_dim=in_neurons, return_sequences=True, init='uniform',\n",
    "                   consume_less=gpu_cpu))\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "dense_input = inner_hidden_neurons\n",
    "model.add(LSTM(output_dim=inner_hidden_neurons, input_dim=hidden_neurons, return_sequences=True, consume_less=gpu_cpu))\n",
    "model.add(Dropout(dropout_inner))\n",
    "model.add(LSTM(input_dim=hidden_neurons, output_dim=dense_input, return_sequences=False))\n",
    "model.add(Dropout(dropout_inner))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(output_dim=out_neurons, input_dim=dense_input))\n",
    "model.add(Activation('relu'))\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.lr = 0.001\n",
    "model.fit(trainX, trainY, nb_epoch=50, batch_size=batch_size, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainPredict = model.predict(trainX, batch_size=batch_size)\n",
    "# model.reset_states()\n",
    "# testPredict = model.predict(testX, batch_size=batch_size)\n",
    "trainPredict = trainScaler.inverse_transform(trainPredict)\n",
    "trainYOri = trainScaler.inverse_transform(trainY)\n",
    "# testPredict = testScaler.inverse_transform(testPredict)\n",
    "# testY = testScaler.inverse_transform(testY)\n",
    "trainScore = mape(trainPredict[:, 0], trainYOri)\n",
    "print('Train Score: %.2f MAPE' % (trainScore))\n",
    "\n",
    "# testScore = mape(testPredict[:, 0], testY)\n",
    "# print('Test Score %.2f MAPE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.legend(loc='best')\n",
    "plt.plot(trainPredict[100:200], label='predict')\n",
    "plt.plot(trainYOri[100:200], label='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation -  One Step Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_predict = np.empty((0,0))\n",
    "all_Y = np.empty((0,0))\n",
    "all_time = []\n",
    "for day in range(11, 18):\n",
    "    for hour in [8, 17]:\n",
    "        train_window_start = datetime(2016, 10, day, hour, 0) - timedelta(hours=2)\n",
    "        train_window_end = train_window_start + timedelta(hours=1, minutes=59)\n",
    "        predict_window_end = datetime(2016, 10, day, hour, 0) + timedelta(hours=2)\n",
    "        cvX, cvY, cvScaler, cvYTime, cvDF = create_dataset(cv[train_window_start:predict_window_end], 'B-3', travel_time_feature, 6)\n",
    "        cvX = np.reshape(cvX, (cvX.shape[0], cvX.shape[1], cvX.shape[2]))\n",
    "        cvPredict = model.predict(cvX)\n",
    "        cvPredict = cvScaler.inverse_transform(cvPredict)\n",
    "        cvY = cvScaler.inverse_transform(cvY)\n",
    "        if all_predict.any():\n",
    "            all_predict = np.vstack((all_predict, cvPredict))\n",
    "            all_Y = np.vstack((all_Y, cvY))\n",
    "        else:\n",
    "            all_predict = cvPredict\n",
    "            all_Y = cvY\n",
    "            \n",
    "        all_time = all_time + list(np.reshape(cvYTime, -1))\n",
    "\n",
    "print \"cv MAPE: %f\" % (mape(all_predict, all_Y))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1,)\n",
    "# pd.DataFrame(all_predict, index=all_time).resample('20min').mean().plot(label='predict', ax=ax)\n",
    "# pd.DataFrame(all_Y, index=all_time).resample('20min').mean().plot(ax=ax)\n",
    "\n",
    "ax.plot(all_predict, label=\"predict\")\n",
    "ax.plot(all_Y, label=\"true\")\n",
    "ticks = [i * len(all_Y) / 10 for i in range(10)]\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_xticklabels([all_time[t].strftime(\"%m-%d %H:%M\") for t in ticks])\n",
    "# plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "# sample_df = pd.DataFrame(dataset[:, 0], index=dataset_time_index, columns=[\"speed\"])\n",
    "# o = sm.tsa.arma_order_select_ic(sample_df['speed'],ic='aic')['aic_min_order']\n",
    "out_file_path = 'arma_test_20min_avg_travel_time.csv'\n",
    "fw = open(out_file_path, 'w')\n",
    "fw.writelines(','.join(['intersection_id', 'tollgate_id', 'time_window', 'avg_travel_time']) + '\\n')\n",
    "for route in routes:\n",
    "    trainX, trainY, trainScaler, trainYTimeIndex, trainDF = create_dataset(time_train, route, travel_time_feature, 1)\n",
    "    o = sm.tsa.arma_order_select_ic(trainDF['speed'], ic='aic')['aic_min_order']\n",
    "    print o\n",
    "    # arma_mod22 = sm.tsa.ARMA(sample_df['speed'], order=o).fit()\n",
    "    arma_mod22 = sm.tsa.ARMA(trainDF['speed'], order=o).fit()\n",
    "    # st = datetime.datetime.strptime('201408241100', '%Y%m%d%H%M')\n",
    "    # y_predict = arma_mod22.predict(start=st.strftime('%Y%m%d%H%M'), end=(st+datetime.timedelta(minutes=30)).strftime('%Y%m%d%H%M'), dynamic=True)\n",
    "    # y_predict = arma_mod22.predict(start=36*21, end=36*24, dynamic=True)\n",
    "    # sm.graphics.tsa.plot_pacf(ori_dataset, lags=50)\n",
    "    for day in range(18, 25):\n",
    "        for hour in [8, 17]:\n",
    "            time_window_start = datetime(2016, 10, day, hour, 0)\n",
    "            time_window_end = time_window_start + timedelta(hours=2)\n",
    "            predicted = arma_mod22.predict(time_window_start, end=time_window_end, dynamic=False)\n",
    "            for t in pd.date_range(time_window_start, end=time_window_end, freq='20min', closed='left'):\n",
    "                avg_tt = predicted[t]\n",
    "                out_line = ','.join([route.split('-')[0], route.split('-')[1],\n",
    "                                 '\"[' + str(t) + ',' + str(t + timedelta(minutes=20)) + ')\"',\n",
    "                                 str(avg_tt)]) + '\\n'\n",
    "                fw.writelines(out_line)\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "online_test_set = pd.read_csv(\"dataSets/testing_phase1/trajectories(table 5)_test1.csv\", header=0)\n",
    "for day in range(18, 25):\n",
    "    for hour in [8, 15]:\n",
    "        time_window_start = datetime(2016, 10, day, hour, 0)\n",
    "        time_window_end = time_window_start + timedelta(hours=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
